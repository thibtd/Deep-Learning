{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural networks from scratch in pytorch \n",
    "\n",
    "Here we explore the different ways to build models using pytorch. How to make a neural net, train it, save it's weights etc. The goal is to build up eperience with the library and then be able to rebuild the basic transformer architecture from Attention is All You Need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things consistent we will keep on using the stanfordnlp/imdb dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/codespace/.python/current/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/codespace/.python/current/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install torch torchvision torchaudio\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from spacy.lang.en import English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\").to_pandas()\n",
    "test_dataset  = load_dataset(\"stanfordnlp/imdb\", split=\"test\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "We keep the punctuation and do not lowercase the text as it can remove useful information\n",
    "1. tokenization \n",
    "2. stop word removal \n",
    "3. lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/codespace/.python/current/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text (remove italic and so on)\n",
    "def clean(text):\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['cleaned_text'] = train_dataset.apply(lambda x: clean(x['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m----> 2\u001b[0m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_token\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m----> 2\u001b[0m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_token\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/nltk/tokenize/destructive.py:182\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    179\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 182\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    184\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "train_dataset['text_token'] = train_dataset.apply(lambda x: word_tokenize(x['cleaned_text']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['text_nostop'] = train_dataset.apply(lambda x: remove_stopwords(x['text_token']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'rented',\n",
       " 'I',\n",
       " 'AM',\n",
       " 'CURIOUS-YELLOW',\n",
       " 'video',\n",
       " 'store',\n",
       " 'controversy',\n",
       " 'surrounded',\n",
       " 'first',\n",
       " 'released',\n",
       " '1967',\n",
       " '.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'heard',\n",
       " 'first',\n",
       " 'seized',\n",
       " 'U.S.',\n",
       " 'customs',\n",
       " 'ever',\n",
       " 'tried',\n",
       " 'enter',\n",
       " 'country',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'fan',\n",
       " 'films',\n",
       " 'considered',\n",
       " '``',\n",
       " 'controversial',\n",
       " \"''\",\n",
       " 'I',\n",
       " 'really',\n",
       " 'see',\n",
       " '.',\n",
       " 'The',\n",
       " 'plot',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'young',\n",
       " 'Swedish',\n",
       " 'drama',\n",
       " 'student',\n",
       " 'named',\n",
       " 'Lena',\n",
       " 'wants',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'life',\n",
       " '.',\n",
       " 'In',\n",
       " 'particular',\n",
       " 'wants',\n",
       " 'focus',\n",
       " 'attentions',\n",
       " 'making',\n",
       " 'sort',\n",
       " 'documentary',\n",
       " 'average',\n",
       " 'Swede',\n",
       " 'thought',\n",
       " 'certain',\n",
       " 'political',\n",
       " 'issues',\n",
       " 'Vietnam',\n",
       " 'War',\n",
       " 'race',\n",
       " 'issues',\n",
       " 'United',\n",
       " 'States',\n",
       " '.',\n",
       " 'In',\n",
       " 'asking',\n",
       " 'politicians',\n",
       " 'ordinary',\n",
       " 'denizens',\n",
       " 'Stockholm',\n",
       " 'opinions',\n",
       " 'politics',\n",
       " ',',\n",
       " 'sex',\n",
       " 'drama',\n",
       " 'teacher',\n",
       " ',',\n",
       " 'classmates',\n",
       " ',',\n",
       " 'married',\n",
       " 'men',\n",
       " '.',\n",
       " 'What',\n",
       " 'kills',\n",
       " 'I',\n",
       " 'AM',\n",
       " 'CURIOUS-YELLOW',\n",
       " '40',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'considered',\n",
       " 'pornographic',\n",
       " '.',\n",
       " 'Really',\n",
       " ',',\n",
       " 'sex',\n",
       " 'nudity',\n",
       " 'scenes',\n",
       " 'far',\n",
       " ',',\n",
       " 'even',\n",
       " \"'s\",\n",
       " 'shot',\n",
       " 'like',\n",
       " 'cheaply',\n",
       " 'made',\n",
       " 'porno',\n",
       " '.',\n",
       " 'While',\n",
       " 'countrymen',\n",
       " 'mind',\n",
       " 'find',\n",
       " 'shocking',\n",
       " ',',\n",
       " 'reality',\n",
       " 'sex',\n",
       " 'nudity',\n",
       " 'major',\n",
       " 'staple',\n",
       " 'Swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'Even',\n",
       " 'Ingmar',\n",
       " 'Bergman',\n",
       " ',',\n",
       " 'arguably',\n",
       " 'answer',\n",
       " 'good',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'John',\n",
       " 'Ford',\n",
       " ',',\n",
       " 'sex',\n",
       " 'scenes',\n",
       " 'films',\n",
       " '.',\n",
       " 'I',\n",
       " 'commend',\n",
       " 'filmmakers',\n",
       " 'fact',\n",
       " 'sex',\n",
       " 'shown',\n",
       " 'film',\n",
       " 'shown',\n",
       " 'artistic',\n",
       " 'purposes',\n",
       " 'rather',\n",
       " 'shock',\n",
       " 'people',\n",
       " 'make',\n",
       " 'money',\n",
       " 'shown',\n",
       " 'pornographic',\n",
       " 'theaters',\n",
       " 'America',\n",
       " '.',\n",
       " 'I',\n",
       " 'AM',\n",
       " 'CURIOUS-YELLOW',\n",
       " 'good',\n",
       " 'film',\n",
       " 'anyone',\n",
       " 'wanting',\n",
       " 'study',\n",
       " 'meat',\n",
       " 'potatoes',\n",
       " '(',\n",
       " 'pun',\n",
       " 'intended',\n",
       " ')',\n",
       " 'Swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'But',\n",
       " 'really',\n",
       " ',',\n",
       " 'film',\n",
       " \"n't\",\n",
       " 'much',\n",
       " 'plot',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text_nostop'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['lemm'] = train_dataset.apply(lambda x: lemmatizer(x['text_nostop']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            I rented I AM CURIOUS-YELLOW from my video sto...\n",
       "label                                                           0\n",
       "cleaned_text    I rented I AM CURIOUS-YELLOW from my video sto...\n",
       "text_token      [I, rented, I, AM, CURIOUS-YELLOW, from, my, v...\n",
       "text_nostop     [I, rented, I, AM, CURIOUS-YELLOW, video, stor...\n",
       "lemm            [I, rented, I, AM, CURIOUS-YELLOW, video, stor...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf to have vectors \n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "train_dataset_tfidf = vectorizer.fit_transform(train_dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['cleaned_text'] = test_dataset.apply(lambda x: clean(x['text']),axis=1)\n",
    "test_dataset_tfidf = vectorizer.fit_transform(test_dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_tfidf.shape)\n",
    "print(test_dataset_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor(train_dataset_tfidf.toarray(), dtype=torch.float32)  # Convert sparse matrix if needed\n",
    "train_y = torch.tensor(train_dataset['label'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = torch.tensor(test_dataset_tfidf.toarray(), dtype=torch.float32)  # Convert sparse matrix if needed\n",
    "test_y = torch.tensor(test_dataset['label'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(train_X,train_y), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating models  \n",
    "1. Make a class that inherits from nn.Module \n",
    "2. in __init__ call super and define the flow of data through the layers. \n",
    "3. To make it faster add a call to check if there is gpu/mps or smtg available. If not then cpu will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_X.shape[1]\n",
    "output_size = 2\n",
    "hidden_size = 265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(input_size=input_size,output_size=output_size,hidden_size=hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=5000, out_features=265, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=265, out_features=265, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=265, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.679658  [   64/25000]\n",
      "loss: 0.000015  [ 6464/25000]\n",
      "loss: 7.123849  [12864/25000]\n",
      "loss: 0.000351  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 5.862230 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 13.377934  [   64/25000]\n",
      "loss: 0.000035  [ 6464/25000]\n",
      "loss: 7.943009  [12864/25000]\n",
      "loss: 0.000175  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 6.477957 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 14.500959  [   64/25000]\n",
      "loss: 0.000097  [ 6464/25000]\n",
      "loss: 9.640593  [12864/25000]\n",
      "loss: 0.012655  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.594038 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 6.081982  [   64/25000]\n",
      "loss: 0.000673  [ 6464/25000]\n",
      "loss: 8.111003  [12864/25000]\n",
      "loss: 0.007384  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 3.184626 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 5.811327  [   64/25000]\n",
      "loss: 0.010858  [ 6464/25000]\n",
      "loss: 4.305522  [12864/25000]\n",
      "loss: 0.004250  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 3.874496 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, network, loss_fn, optimizer)\n",
    "    test(test_loader, network, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0:'negative',1:'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x,y)->dict[float,str]:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        logits = network(x)\n",
    "        probabilities = torch.nn.functional.softmax(logits,dim=0)\n",
    "        pred = probabilities.argmax(0).item()\n",
    "        predicted = classes[pred]\n",
    "        return {'probability':pred,'label':predicted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability: 1, predicted: positive, actual: negative\n"
     ]
    }
   ],
   "source": [
    "x, y = test_X[0], test_dataset['label'].iloc[0]\n",
    "out= inference(x,y)\n",
    "print(f'probability: {out['probability']}, predicted: {out['label']}, actual: {classes[y]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skorch in /home/codespace/.python/current/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from skorch) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from skorch) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from skorch) (1.14.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/codespace/.python/current/lib/python3.12/site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from skorch) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.0->skorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.0->skorch) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "net = NeuralNetClassifier(\n",
    "    network,\n",
    "    max_epochs=10,\n",
    "    criterion=loss_fn,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False, verbose=1)\n",
    "params = {\n",
    "    'lr': np.linspace(1e-5,1e-1,num=10),\n",
    "    'max_epochs': [10],\n",
    "    'module__input_size':[input_size],\n",
    "    'module__hidden_size':[128, 256, 512],\n",
    "    'module__output_size': [output_size]\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6956\u001b[0m  0.5790\n",
      "      2        \u001b[36m0.6956\u001b[0m  0.5497\n",
      "      3        \u001b[36m0.6956\u001b[0m  0.5209\n",
      "      4        \u001b[36m0.6956\u001b[0m  0.5229\n",
      "      5        \u001b[36m0.6956\u001b[0m  0.5173\n",
      "      6        \u001b[36m0.6956\u001b[0m  0.5147\n",
      "      7        \u001b[36m0.6956\u001b[0m  0.4996\n",
      "      8        \u001b[36m0.6956\u001b[0m  0.5168\n",
      "      9        \u001b[36m0.6956\u001b[0m  0.5158\n",
      "     10        \u001b[36m0.6956\u001b[0m  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6941\u001b[0m  0.5464\n",
      "      2        \u001b[36m0.6941\u001b[0m  0.5323\n",
      "      3        \u001b[36m0.6941\u001b[0m  0.5219\n",
      "      4        \u001b[36m0.6941\u001b[0m  0.4991\n",
      "      5        \u001b[36m0.6941\u001b[0m  0.4987\n",
      "      6        \u001b[36m0.6941\u001b[0m  0.5172\n",
      "      7        \u001b[36m0.6941\u001b[0m  0.5520\n",
      "      8        \u001b[36m0.6941\u001b[0m  0.4967\n",
      "      9        \u001b[36m0.6941\u001b[0m  0.5013\n",
      "     10        \u001b[36m0.6941\u001b[0m  0.5206\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6936\u001b[0m  0.5297\n",
      "      2        \u001b[36m0.6936\u001b[0m  0.5412\n",
      "      3        \u001b[36m0.6936\u001b[0m  0.5053\n",
      "      4        \u001b[36m0.6936\u001b[0m  0.5268\n",
      "      5        \u001b[36m0.6936\u001b[0m  0.5057\n",
      "      6        \u001b[36m0.6936\u001b[0m  0.5052\n",
      "      7        \u001b[36m0.6936\u001b[0m  0.5578\n",
      "      8        \u001b[36m0.6936\u001b[0m  0.5124\n",
      "      9        \u001b[36m0.6936\u001b[0m  0.5517\n",
      "     10        \u001b[36m0.6936\u001b[0m  0.4957\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6940\u001b[0m  0.8995\n",
      "      2        \u001b[36m0.6940\u001b[0m  0.8453\n",
      "      3        \u001b[36m0.6940\u001b[0m  0.8725\n",
      "      4        \u001b[36m0.6940\u001b[0m  0.8795\n",
      "      5        \u001b[36m0.6940\u001b[0m  0.8482\n",
      "      6        \u001b[36m0.6940\u001b[0m  0.8967\n",
      "      7        \u001b[36m0.6940\u001b[0m  0.8755\n",
      "      8        \u001b[36m0.6940\u001b[0m  0.8506\n",
      "      9        \u001b[36m0.6940\u001b[0m  0.8444\n",
      "     10        \u001b[36m0.6940\u001b[0m  0.8701\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6936\u001b[0m  0.8332\n",
      "      2        \u001b[36m0.6936\u001b[0m  0.8691\n",
      "      3        \u001b[36m0.6936\u001b[0m  0.8911\n",
      "      4        \u001b[36m0.6936\u001b[0m  0.8510\n",
      "      5        \u001b[36m0.6936\u001b[0m  0.9003\n",
      "      6        \u001b[36m0.6936\u001b[0m  0.8329\n",
      "      7        \u001b[36m0.6936\u001b[0m  0.9344\n",
      "      8        \u001b[36m0.6936\u001b[0m  1.5376\n",
      "      9        \u001b[36m0.6936\u001b[0m  0.8953\n",
      "     10        \u001b[36m0.6936\u001b[0m  0.8755\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6933\u001b[0m  0.8434\n",
      "      2        \u001b[36m0.6933\u001b[0m  0.8410\n",
      "      3        \u001b[36m0.6933\u001b[0m  0.8702\n",
      "      4        \u001b[36m0.6933\u001b[0m  0.8552\n",
      "      5        \u001b[36m0.6933\u001b[0m  0.8550\n",
      "      6        \u001b[36m0.6933\u001b[0m  0.9217\n",
      "      7        \u001b[36m0.6933\u001b[0m  0.8498\n",
      "      8        \u001b[36m0.6933\u001b[0m  0.8609\n",
      "      9        \u001b[36m0.6933\u001b[0m  0.8540\n",
      "     10        \u001b[36m0.6933\u001b[0m  0.8398\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6934\u001b[0m  1.5892\n",
      "      2        \u001b[36m0.6934\u001b[0m  1.5660\n",
      "      3        \u001b[36m0.6934\u001b[0m  1.5861\n",
      "      4        \u001b[36m0.6934\u001b[0m  1.6700\n",
      "      5        \u001b[36m0.6934\u001b[0m  1.5714\n",
      "      6        \u001b[36m0.6934\u001b[0m  1.5761\n",
      "      7        \u001b[36m0.6934\u001b[0m  1.6184\n",
      "      8        \u001b[36m0.6934\u001b[0m  1.6025\n",
      "      9        \u001b[36m0.6934\u001b[0m  1.5818\n",
      "     10        \u001b[36m0.6934\u001b[0m  1.5876\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6931\u001b[0m  1.5958\n",
      "      2        \u001b[36m0.6931\u001b[0m  1.5771\n",
      "      3        \u001b[36m0.6931\u001b[0m  1.5755\n",
      "      4        \u001b[36m0.6931\u001b[0m  1.9646\n",
      "      5        \u001b[36m0.6931\u001b[0m  1.5816\n",
      "      6        \u001b[36m0.6931\u001b[0m  1.6412\n",
      "      7        \u001b[36m0.6931\u001b[0m  1.5921\n",
      "      8        \u001b[36m0.6931\u001b[0m  1.5768\n",
      "      9        \u001b[36m0.6931\u001b[0m  1.5607\n",
      "     10        \u001b[36m0.6931\u001b[0m  1.5589\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6931\u001b[0m  1.5910\n",
      "      2        \u001b[36m0.6931\u001b[0m  1.5386\n",
      "      3        \u001b[36m0.6931\u001b[0m  1.6142\n",
      "      4        \u001b[36m0.6931\u001b[0m  1.5831\n",
      "      5        \u001b[36m0.6931\u001b[0m  1.5465\n",
      "      6        \u001b[36m0.6931\u001b[0m  1.5344\n",
      "      7        \u001b[36m0.6931\u001b[0m  1.5894\n",
      "      8        \u001b[36m0.6931\u001b[0m  1.5600\n",
      "      9        \u001b[36m0.6931\u001b[0m  1.5855\n",
      "     10        \u001b[36m0.6931\u001b[0m  1.5741\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6632\u001b[0m  0.5254\n",
      "      2        0.6740  0.4988\n",
      "      3        0.6823  0.5046\n",
      "      4        0.6863  0.5062\n",
      "      5        0.6879  0.4993\n",
      "      6        0.6886  0.4945\n",
      "      7        0.6888  0.4962\n",
      "      8        0.6889  0.5067\n",
      "      9        0.6890  0.4829\n",
      "     10        0.6890  0.5176\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6600\u001b[0m  0.5148\n",
      "      2        0.6750  0.4873\n",
      "      3        0.6826  0.4948\n",
      "      4        0.6857  0.5091\n",
      "      5        0.6868  0.5754\n",
      "      6        0.6871  0.4975\n",
      "      7        0.6873  0.5074\n",
      "      8        0.6873  0.5105\n",
      "      9        0.6873  0.5326\n",
      "     10        0.6873  0.4956\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6648\u001b[0m  0.5078\n",
      "      2        0.6761  0.5060\n",
      "      3        0.6833  0.4933\n",
      "      4        0.6866  0.4851\n",
      "      5        0.6879  0.4844\n",
      "      6        0.6885  0.4852\n",
      "      7        0.6887  0.5064\n",
      "      8        0.6887  0.4834\n",
      "      9        0.6887  0.5170\n",
      "     10        0.6887  0.4892\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6651\u001b[0m  0.8563\n",
      "      2        0.6766  0.8285\n",
      "      3        0.6836  0.8711\n",
      "      4        0.6866  0.8198\n",
      "      5        0.6878  0.8329\n",
      "      6        0.6883  0.8217\n",
      "      7        0.6885  0.8408\n",
      "      8        0.6886  0.8301\n",
      "      9        0.6886  0.8206\n",
      "     10        0.6886  0.8274\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6621\u001b[0m  0.8291\n",
      "      2        0.6755  0.8269\n",
      "      3        0.6830  0.8387\n",
      "      4        0.6860  0.8271\n",
      "      5        0.6872  0.8748\n",
      "      6        0.6876  0.8268\n",
      "      7        0.6877  0.8379\n",
      "      8        0.6878  0.8152\n",
      "      9        0.6878  0.8236\n",
      "     10        0.6878  0.8527\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6591\u001b[0m  0.8152\n",
      "      2        0.6740  0.8175\n",
      "      3        0.6823  0.8217\n",
      "      4        0.6856  0.8091\n",
      "      5        0.6868  0.8195\n",
      "      6        0.6873  0.8312\n",
      "      7        0.6874  0.8993\n",
      "      8        0.6874  0.8164\n",
      "      9        0.6874  0.8450\n",
      "     10        0.6874  0.8263\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6631\u001b[0m  1.5636\n",
      "      2        0.6760  1.5908\n",
      "      3        0.6833  1.5632\n",
      "      4        0.6863  1.5654\n",
      "      5        0.6875  1.6493\n",
      "      6        0.6879  1.6309\n",
      "      7        0.6881  1.5727\n",
      "      8        0.6881  1.5885\n",
      "      9        0.6881  1.6043\n",
      "     10        0.6881  1.5689\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6612\u001b[0m  1.6221\n",
      "      2        0.6741  1.6492\n",
      "      3        0.6824  1.5627\n",
      "      4        0.6859  1.5768\n",
      "      5        0.6873  1.6224\n",
      "      6        0.6878  1.5789\n",
      "      7        0.6880  1.6623\n",
      "      8        0.6881  1.6309\n",
      "      9        0.6881  1.5708\n",
      "     10        0.6880  1.5754\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6615\u001b[0m  1.6073\n",
      "      2        0.6738  1.5752\n",
      "      3        0.6822  1.5967\n",
      "      4        0.6860  1.5774\n",
      "      5        0.6874  1.6040\n",
      "      6        0.6880  1.5718\n",
      "      7        0.6882  1.5724\n",
      "      8        0.6882  1.6001\n",
      "      9        0.6882  1.5605\n",
      "     10        0.6882  1.6267\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5952\u001b[0m  0.4868\n",
      "      2        0.6521  0.4996\n",
      "      3        0.6648  0.5086\n",
      "      4        0.6670  0.5203\n",
      "      5        0.6674  0.4895\n",
      "      6        0.6673  0.5003\n",
      "      7        0.6673  0.4989\n",
      "      8        0.6671  0.4966\n",
      "      9        0.6669  0.4879\n",
      "     10        0.6666  0.5125\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6084\u001b[0m  0.4955\n",
      "      2        0.6571  0.4898\n",
      "      3        0.6683  0.5180\n",
      "      4        0.6703  0.5279\n",
      "      5        0.6708  0.5000\n",
      "      6        0.6709  0.4900\n",
      "      7        0.6710  0.5284\n",
      "      8        0.6709  0.5119\n",
      "      9        0.6708  0.5245\n",
      "     10        0.6705  0.5125\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6004\u001b[0m  0.4901\n",
      "      2        0.6527  0.4981\n",
      "      3        0.6645  0.4898\n",
      "      4        0.6666  0.5283\n",
      "      5        0.6671  0.4870\n",
      "      6        0.6673  0.5262\n",
      "      7        0.6674  0.4892\n",
      "      8        0.6674  0.4921\n",
      "      9        0.6674  0.5196\n",
      "     10        0.6673  0.5037\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6064\u001b[0m  0.8204\n",
      "      2        0.6559  0.8403\n",
      "      3        0.6673  0.8410\n",
      "      4        0.6695  0.8678\n",
      "      5        0.6700  0.8285\n",
      "      6        0.6702  0.8442\n",
      "      7        0.6703  0.8240\n",
      "      8        0.6703  0.8113\n",
      "      9        0.6703  0.8271\n",
      "     10        0.6703  0.8448\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5981\u001b[0m  0.8589\n",
      "      2        0.6522  0.8499\n",
      "      3        0.6650  0.8260\n",
      "      4        0.6672  0.8375\n",
      "      5        0.6678  0.8472\n",
      "      6        0.6680  0.9106\n",
      "      7        0.6682  0.8493\n",
      "      8        0.6683  0.8228\n",
      "      9        0.6683  0.8371\n",
      "     10        0.6683  0.8259\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5995\u001b[0m  0.8583\n",
      "      2        0.6542  0.8564\n",
      "      3        0.6661  0.8887\n",
      "      4        0.6683  0.8124\n",
      "      5        0.6687  0.8133\n",
      "      6        0.6689  0.8318\n",
      "      7        0.6690  0.8296\n",
      "      8        0.6691  0.9124\n",
      "      9        0.6691  0.8486\n",
      "     10        0.6690  0.8189\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5952\u001b[0m  1.5625\n",
      "      2        0.6514  1.5940\n",
      "      3        0.6636  1.5965\n",
      "      4        0.6658  1.5774\n",
      "      5        0.6664  1.6107\n",
      "      6        0.6667  1.6107\n",
      "      7        0.6669  1.5731\n",
      "      8        0.6670  1.5822\n",
      "      9        0.6671  1.6219\n",
      "     10        0.6671  1.5827\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5920\u001b[0m  1.5999\n",
      "      2        0.6510  1.6054\n",
      "      3        0.6641  1.5839\n",
      "      4        0.6663  1.5826\n",
      "      5        0.6668  1.5926\n",
      "      6        0.6671  1.6238\n",
      "      7        0.6672  1.5949\n",
      "      8        0.6672  1.7020\n",
      "      9        0.6671  1.6086\n",
      "     10        0.6670  1.5797\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5964\u001b[0m  1.5784\n",
      "      2        0.6520  1.6129\n",
      "      3        0.6640  1.5763\n",
      "      4        0.6661  1.6220\n",
      "      5        0.6665  1.5806\n",
      "      6        0.6667  1.6581\n",
      "      7        0.6668  1.5945\n",
      "      8        0.6668  1.5812\n",
      "      9        0.6667  1.6568\n",
      "     10        0.6665  1.5993\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5447\u001b[0m  0.4918\n",
      "      2        0.6296  0.5076\n",
      "      3        0.6387  0.4926\n",
      "      4        0.6404  0.5033\n",
      "      5        0.6414  0.5141\n",
      "      6        0.6421  0.5292\n",
      "      7        0.6426  0.4999\n",
      "      8        0.6429  0.5073\n",
      "      9        0.6429  0.5012\n",
      "     10        0.6427  0.5237\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5368\u001b[0m  0.5114\n",
      "      2        0.6263  0.5242\n",
      "      3        0.6364  0.4979\n",
      "      4        0.6385  0.4986\n",
      "      5        0.6397  0.5205\n",
      "      6        0.6405  0.5315\n",
      "      7        0.6410  0.4976\n",
      "      8        0.6411  0.5464\n",
      "      9        0.6408  0.5112\n",
      "     10        0.6403  0.5085\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5315\u001b[0m  0.5032\n",
      "      2        0.6258  0.4961\n",
      "      3        0.6364  0.5133\n",
      "      4        0.6383  0.4990\n",
      "      5        0.6392  0.5122\n",
      "      6        0.6399  0.5272\n",
      "      7        0.6402  0.5179\n",
      "      8        0.6401  0.5009\n",
      "      9        0.6399  0.5172\n",
      "     10        0.6393  0.4963\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5367\u001b[0m  0.8442\n",
      "      2        0.6282  0.8251\n",
      "      3        0.6389  0.8397\n",
      "      4        0.6406  0.8477\n",
      "      5        0.6416  0.8766\n",
      "      6        0.6422  0.8219\n",
      "      7        0.6427  0.8266\n",
      "      8        0.6429  0.8309\n",
      "      9        0.6431  0.8314\n",
      "     10        0.6429  0.8488\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5387\u001b[0m  0.8405\n",
      "      2        0.6298  0.8623\n",
      "      3        0.6402  0.8304\n",
      "      4        0.6421  0.8503\n",
      "      5        0.6428  0.8365\n",
      "      6        0.6431  0.8618\n",
      "      7        0.6434  0.8744\n",
      "      8        0.6434  0.8182\n",
      "      9        0.6432  0.8281\n",
      "     10        0.6427  0.8524\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5283\u001b[0m  0.8333\n",
      "      2        0.6265  0.8663\n",
      "      3        0.6370  0.8555\n",
      "      4        0.6385  0.8392\n",
      "      5        0.6394  0.8149\n",
      "      6        0.6401  0.8380\n",
      "      7        0.6404  0.8237\n",
      "      8        0.6407  0.8667\n",
      "      9        0.6406  0.8669\n",
      "     10        0.6403  0.8209\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5359\u001b[0m  1.5887\n",
      "      2        0.6277  1.5902\n",
      "      3        0.6380  1.6125\n",
      "      4        0.6398  1.5664\n",
      "      5        0.6409  1.5804\n",
      "      6        0.6417  1.6318\n",
      "      7        0.6423  1.5893\n",
      "      8        0.6427  1.5721\n",
      "      9        0.6428  1.5836\n",
      "     10        0.6427  1.6729\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5376\u001b[0m  1.5842\n",
      "      2        0.6281  1.6160\n",
      "      3        0.6379  1.6115\n",
      "      4        0.6397  1.5821\n",
      "      5        0.6406  1.5753\n",
      "      6        0.6413  1.5827\n",
      "      7        0.6416  1.5778\n",
      "      8        0.6417  1.6174\n",
      "      9        0.6415  1.5634\n",
      "     10        0.6410  1.6102\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5347\u001b[0m  1.5977\n",
      "      2        0.6255  1.5890\n",
      "      3        0.6348  1.6186\n",
      "      4        0.6366  1.6626\n",
      "      5        0.6378  1.5944\n",
      "      6        0.6384  1.6182\n",
      "      7        0.6388  1.6108\n",
      "      8        0.6389  1.5718\n",
      "      9        0.6387  1.5979\n",
      "     10        0.6381  1.6260\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4753\u001b[0m  0.5024\n",
      "      2        0.5947  0.4951\n",
      "      3        0.6032  0.5042\n",
      "      4        0.6066  0.5042\n",
      "      5        0.6089  0.5021\n",
      "      6        0.6101  0.5023\n",
      "      7        0.6107  0.5251\n",
      "      8        0.6108  0.5003\n",
      "      9        0.6102  0.5390\n",
      "     10        0.6089  0.4898\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4782\u001b[0m  0.5018\n",
      "      2        0.5963  0.4922\n",
      "      3        0.6041  0.5165\n",
      "      4        0.6061  0.5044\n",
      "      5        0.6075  0.4968\n",
      "      6        0.6076  0.4920\n",
      "      7        0.6074  0.5310\n",
      "      8        0.6062  0.5166\n",
      "      9        0.6044  0.5906\n",
      "     10        0.6014  0.5296\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4747\u001b[0m  0.5016\n",
      "      2        0.5953  0.4941\n",
      "      3        0.6036  0.5003\n",
      "      4        0.6064  0.5024\n",
      "      5        0.6085  0.4988\n",
      "      6        0.6100  0.4981\n",
      "      7        0.6108  0.5059\n",
      "      8        0.6112  0.5330\n",
      "      9        0.6111  0.5053\n",
      "     10        0.6099  0.4945\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4822\u001b[0m  0.8456\n",
      "      2        0.5986  0.8446\n",
      "      3        0.6075  0.8309\n",
      "      4        0.6101  0.8407\n",
      "      5        0.6119  0.9211\n",
      "      6        0.6130  0.8434\n",
      "      7        0.6135  0.8413\n",
      "      8        0.6134  0.8363\n",
      "      9        0.6127  0.8364\n",
      "     10        0.6113  0.8257\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4731\u001b[0m  0.8945\n",
      "      2        0.5938  0.8819\n",
      "      3        0.6022  0.8569\n",
      "      4        0.6047  0.8298\n",
      "      5        0.6063  0.8309\n",
      "      6        0.6071  0.8407\n",
      "      7        0.6071  0.9089\n",
      "      8        0.6063  0.8513\n",
      "      9        0.6047  0.8487\n",
      "     10        0.6018  0.8395\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4740\u001b[0m  0.8425\n",
      "      2        0.5984  0.8808\n",
      "      3        0.6074  0.8791\n",
      "      4        0.6101  0.8658\n",
      "      5        0.6117  0.8777\n",
      "      6        0.6127  0.8364\n",
      "      7        0.6131  0.8548\n",
      "      8        0.6127  0.8835\n",
      "      9        0.6118  0.8622\n",
      "     10        0.6102  0.9071\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4733\u001b[0m  1.5759\n",
      "      2        0.5947  1.5825\n",
      "      3        0.6038  1.6166\n",
      "      4        0.6071  1.5762\n",
      "      5        0.6091  1.5846\n",
      "      6        0.6104  1.6261\n",
      "      7        0.6111  1.5984\n",
      "      8        0.6111  1.6252\n",
      "      9        0.6102  1.5852\n",
      "     10        0.6084  1.6137\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4701\u001b[0m  1.5888\n",
      "      2        0.5921  1.6329\n",
      "      3        0.6014  1.5961\n",
      "      4        0.6051  1.6155\n",
      "      5        0.6078  1.5889\n",
      "      6        0.6098  1.5954\n",
      "      7        0.6111  1.6018\n",
      "      8        0.6116  1.6290\n",
      "      9        0.6114  1.5900\n",
      "     10        0.6104  1.6139\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4793\u001b[0m  1.5778\n",
      "      2        0.5992  1.5753\n",
      "      3        0.6087  1.5953\n",
      "      4        0.6116  1.6550\n",
      "      5        0.6137  1.5709\n",
      "      6        0.6151  1.5761\n",
      "      7        0.6161  1.6244\n",
      "      8        0.6165  1.5592\n",
      "      9        0.6165  1.5587\n",
      "     10        0.6158  1.6098\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4364\u001b[0m  0.5110\n",
      "      2        0.5646  0.4970\n",
      "      3        0.5735  0.4928\n",
      "      4        0.5773  0.4879\n",
      "      5        0.5795  0.4954\n",
      "      6        0.5808  0.5023\n",
      "      7        0.5803  0.5012\n",
      "      8        0.5782  0.5131\n",
      "      9        0.5741  0.5191\n",
      "     10        0.5676  0.4948\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4180\u001b[0m  0.5078\n",
      "      2        0.5594  0.4815\n",
      "      3        0.5685  0.4872\n",
      "      4        0.5732  0.4949\n",
      "      5        0.5765  0.4927\n",
      "      6        0.5785  0.4850\n",
      "      7        0.5789  0.5071\n",
      "      8        0.5777  0.5140\n",
      "      9        0.5746  0.5274\n",
      "     10        0.5694  0.5722\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4504\u001b[0m  0.5051\n",
      "      2        0.5735  0.4841\n",
      "      3        0.5825  0.4834\n",
      "      4        0.5863  0.4884\n",
      "      5        0.5888  0.4836\n",
      "      6        0.5905  0.4954\n",
      "      7        0.5915  0.5106\n",
      "      8        0.5915  0.5636\n",
      "      9        0.5905  0.5228\n",
      "     10        0.5887  0.5378\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4216\u001b[0m  0.8367\n",
      "      2        0.5560  0.8378\n",
      "      3        0.5664  0.8251\n",
      "      4        0.5719  0.8534\n",
      "      5        0.5759  0.8463\n",
      "      6        0.5785  0.9037\n",
      "      7        0.5797  0.8451\n",
      "      8        0.5794  0.8266\n",
      "      9        0.5780  0.8406\n",
      "     10        0.5748  0.8387\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4278\u001b[0m  0.8304\n",
      "      2        0.5618  0.8588\n",
      "      3        0.5722  0.8196\n",
      "      4        0.5777  0.8251\n",
      "      5        0.5817  0.8179\n",
      "      6        0.5846  0.8424\n",
      "      7        0.5864  0.8613\n",
      "      8        0.5870  0.8438\n",
      "      9        0.5866  0.8498\n",
      "     10        0.5851  0.8425\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4275\u001b[0m  0.8337\n",
      "      2        0.5627  0.8464\n",
      "      3        0.5729  0.8485\n",
      "      4        0.5783  0.8277\n",
      "      5        0.5821  0.8570\n",
      "      6        0.5846  0.8339\n",
      "      7        0.5859  0.8243\n",
      "      8        0.5859  0.8510\n",
      "      9        0.5846  0.8860\n",
      "     10        0.5820  0.8219\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4372\u001b[0m  1.6095\n",
      "      2        0.5665  1.5827\n",
      "      3        0.5759  1.5654\n",
      "      4        0.5806  1.5873\n",
      "      5        0.5838  1.5827\n",
      "      6        0.5857  1.6165\n",
      "      7        0.5864  1.5903\n",
      "      8        0.5859  1.6351\n",
      "      9        0.5840  1.5955\n",
      "     10        0.5809  1.5703\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4268\u001b[0m  1.5908\n",
      "      2        0.5604  1.6408\n",
      "      3        0.5696  1.6080\n",
      "      4        0.5744  1.6015\n",
      "      5        0.5773  1.5782\n",
      "      6        0.5789  1.5703\n",
      "      7        0.5791  1.5922\n",
      "      8        0.5778  1.6329\n",
      "      9        0.5749  1.6059\n",
      "     10        0.5699  1.5568\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4275\u001b[0m  1.5841\n",
      "      2        0.5627  1.5934\n",
      "      3        0.5727  1.5591\n",
      "      4        0.5781  1.6218\n",
      "      5        0.5819  1.6047\n",
      "      6        0.5842  1.5528\n",
      "      7        0.5853  1.5902\n",
      "      8        0.5851  1.5714\n",
      "      9        0.5836  1.5503\n",
      "     10        0.5807  1.5823\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4012\u001b[0m  0.5444\n",
      "      2        0.5442  0.5214\n",
      "      3        0.5535  0.4998\n",
      "      4        0.5582  0.4929\n",
      "      5        0.5603  0.4864\n",
      "      6        0.5613  0.4837\n",
      "      7        0.5602  0.4857\n",
      "      8        0.5566  0.5142\n",
      "      9        0.5512  0.5205\n",
      "     10        0.5431  0.5061\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3990\u001b[0m  0.5307\n",
      "      2        0.5349  0.5087\n",
      "      3        0.5428  0.5059\n",
      "      4        0.5469  0.5065\n",
      "      5        0.5494  0.4896\n",
      "      6        0.5504  0.4941\n",
      "      7        0.5495  0.5049\n",
      "      8        0.5463  0.5182\n",
      "      9        0.5397  0.4847\n",
      "     10        0.5296  0.5168\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3808\u001b[0m  0.6144\n",
      "      2        0.5228  0.5089\n",
      "      3        0.5324  0.4987\n",
      "      4        0.5388  0.4992\n",
      "      5        0.5444  0.4933\n",
      "      6        0.5473  0.5185\n",
      "      7        0.5479  0.5138\n",
      "      8        0.5462  0.5076\n",
      "      9        0.5411  0.5060\n",
      "     10        0.5323  0.4952\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4013\u001b[0m  0.8645\n",
      "      2        0.5348  0.8232\n",
      "      3        0.5449  0.8322\n",
      "      4        0.5505  0.8493\n",
      "      5        0.5540  0.8263\n",
      "      6        0.5559  0.8552\n",
      "      7        0.5560  0.8641\n",
      "      8        0.5543  0.8898\n",
      "      9        0.5502  0.8197\n",
      "     10        0.5440  0.8311\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3918\u001b[0m  0.8391\n",
      "      2        0.5322  0.8167\n",
      "      3        0.5425  0.8799\n",
      "      4        0.5488  0.8465\n",
      "      5        0.5523  0.8416\n",
      "      6        0.5537  0.8477\n",
      "      7        0.5529  0.8485\n",
      "      8        0.5499  0.8964\n",
      "      9        0.5448  0.8385\n",
      "     10        0.5366  0.8735\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3917\u001b[0m  0.8255\n",
      "      2        0.5349  0.8704\n",
      "      3        0.5466  0.8270\n",
      "      4        0.5535  0.8393\n",
      "      5        0.5585  0.8267\n",
      "      6        0.5617  0.8666\n",
      "      7        0.5631  0.8506\n",
      "      8        0.5628  0.8453\n",
      "      9        0.5608  0.8670\n",
      "     10        0.5565  0.9035\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3876\u001b[0m  1.5826\n",
      "      2        0.5332  1.5877\n",
      "      3        0.5443  1.5528\n",
      "      4        0.5509  1.5446\n",
      "      5        0.5551  1.5948\n",
      "      6        0.5571  1.6456\n",
      "      7        0.5573  1.5746\n",
      "      8        0.5557  1.5852\n",
      "      9        0.5522  1.5732\n",
      "     10        0.5462  1.5704\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3929\u001b[0m  1.6143\n",
      "      2        0.5329  1.6350\n",
      "      3        0.5438  1.5961\n",
      "      4        0.5503  1.6025\n",
      "      5        0.5545  1.6157\n",
      "      6        0.5571  1.5793\n",
      "      7        0.5580  1.5948\n",
      "      8        0.5571  1.6363\n",
      "      9        0.5541  1.6008\n",
      "     10        0.5490  1.5588\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3873\u001b[0m  1.5983\n",
      "      2        0.5319  1.5758\n",
      "      3        0.5430  1.5629\n",
      "      4        0.5497  1.5952\n",
      "      5        0.5539  1.6921\n",
      "      6        0.5560  1.5825\n",
      "      7        0.5560  1.5967\n",
      "      8        0.5541  1.6080\n",
      "      9        0.5500  1.5718\n",
      "     10        0.5432  1.6012\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3607\u001b[0m  0.4923\n",
      "      2        0.5089  0.5495\n",
      "      3        0.5196  0.5386\n",
      "      4        0.5236  0.4986\n",
      "      5        0.5251  0.4934\n",
      "      6        0.5227  0.5078\n",
      "      7        0.5159  0.4943\n",
      "      8        0.5058  0.5150\n",
      "      9        0.4898  0.5056\n",
      "     10        0.4715  0.5035\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3541\u001b[0m  0.5008\n",
      "      2        0.5025  0.4922\n",
      "      3        0.5157  0.5391\n",
      "      4        0.5235  0.5099\n",
      "      5        0.5279  0.5004\n",
      "      6        0.5296  0.4909\n",
      "      7        0.5290  0.5180\n",
      "      8        0.5263  0.5092\n",
      "      9        0.5201  0.5304\n",
      "     10        0.5108  0.5413\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3717\u001b[0m  0.5402\n",
      "      2        0.5152  0.5370\n",
      "      3        0.5237  0.5466\n",
      "      4        0.5277  0.5065\n",
      "      5        0.5292  0.5098\n",
      "      6        0.5286  0.5066\n",
      "      7        0.5253  0.5181\n",
      "      8        0.5193  0.5115\n",
      "      9        0.5106  0.5054\n",
      "     10        0.4980  0.4999\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3602\u001b[0m  0.8280\n",
      "      2        0.5021  0.8642\n",
      "      3        0.5141  0.8199\n",
      "      4        0.5219  0.8635\n",
      "      5        0.5264  0.8181\n",
      "      6        0.5278  0.8159\n",
      "      7        0.5263  0.8710\n",
      "      8        0.5223  0.8344\n",
      "      9        0.5152  0.8451\n",
      "     10        0.5047  0.8446\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3564\u001b[0m  0.8974\n",
      "      2        0.5068  0.8257\n",
      "      3        0.5197  0.8423\n",
      "      4        0.5281  0.8423\n",
      "      5        0.5329  0.8491\n",
      "      6        0.5354  0.8591\n",
      "      7        0.5357  0.8378\n",
      "      8        0.5334  0.8458\n",
      "      9        0.5286  0.8744\n",
      "     10        0.5207  0.8305\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3658\u001b[0m  0.8761\n",
      "      2        0.5140  0.8617\n",
      "      3        0.5251  0.8453\n",
      "      4        0.5319  0.8226\n",
      "      5        0.5363  0.8449\n",
      "      6        0.5386  0.8215\n",
      "      7        0.5386  0.8777\n",
      "      8        0.5363  0.8522\n",
      "      9        0.5314  0.8214\n",
      "     10        0.5235  0.8953\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3601\u001b[0m  1.5895\n",
      "      2        0.5046  1.6076\n",
      "      3        0.5183  1.5593\n",
      "      4        0.5270  1.5656\n",
      "      5        0.5326  1.6362\n",
      "      6        0.5357  1.6044\n",
      "      7        0.5367  1.5890\n",
      "      8        0.5358  1.5842\n",
      "      9        0.5327  1.5905\n",
      "     10        0.5272  1.5574\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3581\u001b[0m  1.5762\n",
      "      2        0.5026  1.5771\n",
      "      3        0.5160  1.5964\n",
      "      4        0.5245  1.5827\n",
      "      5        0.5296  1.5974\n",
      "      6        0.5319  1.5634\n",
      "      7        0.5317  1.5920\n",
      "      8        0.5290  1.5763\n",
      "      9        0.5235  1.6384\n",
      "     10        0.5148  1.5677\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3608\u001b[0m  1.5631\n",
      "      2        0.5048  1.5864\n",
      "      3        0.5176  1.5775\n",
      "      4        0.5255  1.5588\n",
      "      5        0.5302  1.6420\n",
      "      6        0.5321  1.5689\n",
      "      7        0.5310  1.5818\n",
      "      8        0.5270  1.5610\n",
      "      9        0.5203  1.5821\n",
      "     10        0.5102  1.5651\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3334\u001b[0m  0.4904\n",
      "      2        0.4780  0.4879\n",
      "      3        0.4922  0.4890\n",
      "      4        0.5023  0.5559\n",
      "      5        0.5090  0.5240\n",
      "      6        0.5129  0.4895\n",
      "      7        0.5141  0.4917\n",
      "      8        0.5133  0.5254\n",
      "      9        0.5102  0.4895\n",
      "     10        0.5039  0.4903\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3265\u001b[0m  0.5235\n",
      "      2        0.4752  0.4953\n",
      "      3        0.4854  0.4933\n",
      "      4        0.4938  0.4920\n",
      "      5        0.4981  0.5431\n",
      "      6        0.4971  0.5274\n",
      "      7        0.4910  0.5261\n",
      "      8        0.4807  0.4946\n",
      "      9        0.4665  0.5085\n",
      "     10        0.4482  0.5123\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3362\u001b[0m  0.4861\n",
      "      2        0.4860  0.4862\n",
      "      3        0.5015  0.5744\n",
      "      4        0.5103  0.4874\n",
      "      5        0.5159  0.5000\n",
      "      6        0.5185  0.5065\n",
      "      7        0.5184  0.5235\n",
      "      8        0.5160  0.4890\n",
      "      9        0.5099  0.4912\n",
      "     10        0.5002  0.4938\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3304\u001b[0m  0.8324\n",
      "      2        0.4758  0.8237\n",
      "      3        0.4894  0.8379\n",
      "      4        0.4976  0.9182\n",
      "      5        0.5022  0.8287\n",
      "      6        0.5026  0.8321\n",
      "      7        0.5001  0.8233\n",
      "      8        0.4926  0.8943\n",
      "      9        0.4818  0.8197\n",
      "     10        0.4671  0.9045\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3306\u001b[0m  0.8196\n",
      "      2        0.4742  0.8336\n",
      "      3        0.4880  0.8174\n",
      "      4        0.4969  0.8174\n",
      "      5        0.5014  0.8248\n",
      "      6        0.5023  0.8541\n",
      "      7        0.5000  0.8394\n",
      "      8        0.4948  0.8217\n",
      "      9        0.4846  0.8314\n",
      "     10        0.4697  0.8707\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3234\u001b[0m  0.8506\n",
      "      2        0.4772  0.8475\n",
      "      3        0.4921  0.8329\n",
      "      4        0.5015  0.8205\n",
      "      5        0.5066  0.8112\n",
      "      6        0.5084  0.8151\n",
      "      7        0.5073  0.8408\n",
      "      8        0.5032  0.8187\n",
      "      9        0.4953  0.8450\n",
      "     10        0.4834  0.8079\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3355\u001b[0m  1.5708\n",
      "      2        0.4812  1.5568\n",
      "      3        0.4959  1.5564\n",
      "      4        0.5053  1.5406\n",
      "      5        0.5108  1.5589\n",
      "      6        0.5134  1.5493\n",
      "      7        0.5129  1.5391\n",
      "      8        0.5097  1.5774\n",
      "      9        0.5033  1.5325\n",
      "     10        0.4936  1.5461\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3351\u001b[0m  1.5500\n",
      "      2        0.4819  1.5179\n",
      "      3        0.4973  1.5772\n",
      "      4        0.5070  1.5768\n",
      "      5        0.5127  1.5649\n",
      "      6        0.5154  1.5271\n",
      "      7        0.5153  1.5564\n",
      "      8        0.5126  1.5567\n",
      "      9        0.5071  1.5295\n",
      "     10        0.4985  1.5638\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3240\u001b[0m  1.5512\n",
      "      2        0.4722  1.5346\n",
      "      3        0.4879  1.5897\n",
      "      4        0.4980  1.6034\n",
      "      5        0.5041  1.5294\n",
      "      6        0.5068  1.5410\n",
      "      7        0.5060  1.6988\n",
      "      8        0.5019  1.5471\n",
      "      9        0.4943  1.5429\n",
      "     10        0.4831  1.5567\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3131\u001b[0m  0.4872\n",
      "      2        0.4630  0.4824\n",
      "      3        0.4787  0.4885\n",
      "      4        0.4883  0.5152\n",
      "      5        0.4932  0.4858\n",
      "      6        0.4948  0.4900\n",
      "      7        0.4921  0.4911\n",
      "      8        0.4848  0.5457\n",
      "      9        0.4726  0.5020\n",
      "     10        0.4554  0.5340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3116\u001b[0m  0.5066\n",
      "      2        0.4590  0.4900\n",
      "      3        0.4757  0.4901\n",
      "      4        0.4860  0.4885\n",
      "      5        0.4916  0.4916\n",
      "      6        0.4926  0.5038\n",
      "      7        0.4924  0.5009\n",
      "      8        0.4898  0.4861\n",
      "      9        0.4826  0.5006\n",
      "     10        0.4712  0.5877\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3078\u001b[0m  0.4945\n",
      "      2        0.4616  0.4981\n",
      "      3        0.4789  0.4892\n",
      "      4        0.4882  0.5013\n",
      "      5        0.4934  0.4869\n",
      "      6        0.4948  0.4921\n",
      "      7        0.4927  0.5153\n",
      "      8        0.4875  0.5396\n",
      "      9        0.4778  0.5060\n",
      "     10        0.4629  0.5130\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3176\u001b[0m  0.8446\n",
      "      2        0.4608  0.8196\n",
      "      3        0.4769  0.8220\n",
      "      4        0.4876  0.8307\n",
      "      5        0.4940  0.8513\n",
      "      6        0.4969  0.8276\n",
      "      7        0.4970  0.8557\n",
      "      8        0.4940  0.8433\n",
      "      9        0.4882  0.8216\n",
      "     10        0.4789  0.8285\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3052\u001b[0m  0.8401\n",
      "      2        0.4515  0.8452\n",
      "      3        0.4693  0.8762\n",
      "      4        0.4807  0.8369\n",
      "      5        0.4881  0.8374\n",
      "      6        0.4917  0.8381\n",
      "      7        0.4921  0.8334\n",
      "      8        0.4886  0.8327\n",
      "      9        0.4820  0.8576\n",
      "     10        0.4712  0.8488\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3090\u001b[0m  0.8242\n",
      "      2        0.4609  0.9995\n",
      "      3        0.4788  0.8412\n",
      "      4        0.4903  0.8161\n",
      "      5        0.4979  0.8599\n",
      "      6        0.5020  0.8298\n",
      "      7        0.5030  0.8274\n",
      "      8        0.5009  0.8331\n",
      "      9        0.4963  0.8738\n",
      "     10        0.4892  0.8161\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3144\u001b[0m  1.6089\n",
      "      2        0.4571  1.6558\n",
      "      3        0.4720  1.5490\n",
      "      4        0.4813  1.5748\n",
      "      5        0.4861  1.6051\n",
      "      6        0.4870  1.5605\n",
      "      7        0.4842  1.5399\n",
      "      8        0.4782  1.6180\n",
      "      9        0.4686  1.6653\n",
      "     10        0.4560  1.5674\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3111\u001b[0m  1.5830\n",
      "      2        0.4539  1.5587\n",
      "      3        0.4689  1.5379\n",
      "      4        0.4789  1.5855\n",
      "      5        0.4843  1.6823\n",
      "      6        0.4858  1.5781\n",
      "      7        0.4836  1.5341\n",
      "      8        0.4779  1.6382\n",
      "      9        0.4684  1.5589\n",
      "     10        0.4547  1.5516\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3108\u001b[0m  1.6881\n",
      "      2        0.4588  1.5461\n",
      "      3        0.4736  1.5396\n",
      "      4        0.4833  1.5853\n",
      "      5        0.4887  1.5511\n",
      "      6        0.4904  1.5499\n",
      "      7        0.4878  1.5791\n",
      "      8        0.4817  1.6000\n",
      "      9        0.4721  1.5494\n",
      "     10        0.4586  1.5552\n",
      "0.5015200608024322 {'lr': np.float64(1e-05), 'max_epochs': 10, 'module__hidden_size': 512, 'module__input_size': 5000, 'module__output_size': 2}\n"
     ]
    }
   ],
   "source": [
    "train_X_np = train_X.detach().cpu().numpy()\n",
    "train_y_np = train_y.detach().cpu().numpy()\n",
    "gs.fit(train_X_np, train_y_np)\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5015200608024322 {'lr': np.float64(1e-05), 'max_epochs': 10, 'module__hidden_size': 512, 'module__input_size': 5000, 'module__output_size': 2}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_optimized = NeuralNetwork(input_size=gs.best_params_['module__input_size'],hidden_size=gs.best_params_['module__hidden_size'], output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(network.parameters(), lr=gs.best_params_['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 9.176047  [   64/25000]\n",
      "loss: 0.003266  [ 6464/25000]\n",
      "loss: 5.849284  [12864/25000]\n",
      "loss: 0.016837  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 3.709641 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 7.082348  [   64/25000]\n",
      "loss: 0.000132  [ 6464/25000]\n",
      "loss: 6.137697  [12864/25000]\n",
      "loss: 0.011728  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.780824 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 5.097123  [   64/25000]\n",
      "loss: 0.006965  [ 6464/25000]\n",
      "loss: 3.517331  [12864/25000]\n",
      "loss: 0.009049  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.792031 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4.337783  [   64/25000]\n",
      "loss: 0.014396  [ 6464/25000]\n",
      "loss: 1.464381  [12864/25000]\n",
      "loss: 0.024867  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 2.167470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.216345  [   64/25000]\n",
      "loss: 0.016510  [ 6464/25000]\n",
      "loss: 0.967775  [12864/25000]\n",
      "loss: 0.021682  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 3.735809 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.521852  [   64/25000]\n",
      "loss: 0.003387  [ 6464/25000]\n",
      "loss: 2.258009  [12864/25000]\n",
      "loss: 0.009390  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 2.498280 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.720134  [   64/25000]\n",
      "loss: 0.002298  [ 6464/25000]\n",
      "loss: 1.433599  [12864/25000]\n",
      "loss: 0.008899  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 2.480031 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.409150  [   64/25000]\n",
      "loss: 0.001596  [ 6464/25000]\n",
      "loss: 1.780692  [12864/25000]\n",
      "loss: 0.001108  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 3.540769 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.356169  [   64/25000]\n",
      "loss: 0.003561  [ 6464/25000]\n",
      "loss: 1.192017  [12864/25000]\n",
      "loss: 0.002884  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 3.535060 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.677900  [   64/25000]\n",
      "loss: 0.006082  [ 6464/25000]\n",
      "loss: 0.518995  [12864/25000]\n",
      "loss: 0.001782  [19264/25000]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 3.931011 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = gs.best_params_['max_epochs']\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, network, loss_fn, optimizer)\n",
    "    test(test_loader, network, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
